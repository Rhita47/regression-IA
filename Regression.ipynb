{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VkYTax2SeeQt"
   },
   "source": [
    "# Prediction with linear models\n",
    "\n",
    "In this session we will implement models for prediction in high-dimensional environments and study the methodologies involved.\n",
    "\n",
    "\n",
    "We will rely on [Scikit-Learn](http://scikit-learn.org/) for the implementation. Scikit-Learn is a python library that provides a number of tools for machine learning.\n",
    "See [here](http://scikit-learn.org/stable/documentation.html) for more information and documentation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jFCqmWbSeeQt"
   },
   "source": [
    "## Summary and problem set up\n",
    "\n",
    "We are interested in predicting a target variable, $y$, using information from a vector of predictor variables, $\\mathbf{x} = (x_1,\\dots  ,x_p)'$. Our predictions need to be _scored_ by some loss function. A typical choice is the square loss.\n",
    "\n",
    "The _expected (square) loss_, or the _mean squared error_ of a forecast $f({\\bf x})$ is given by\n",
    "\n",
    "$$ \\text{MSE} = \\E\\Big[ \\Big( y - f({\\bf x}) \\Big)^2\\Big] $$\n",
    "\n",
    "It can be easily verified that the function that minimizes this particular loss function is $\\E[ y | {\\bf x}]$ , the conditional expectation of $y$ given ${\\bf x}$.\n",
    "\n",
    "Typically, however, we do not know what $\\E[y | {\\bf x}]$ looks like, and we need to _learn_, or _estimate_ it from the data. A natural starting point is to _assume_ this function can be reasonably approximated by a linear function.\n",
    "\n",
    "To fix ideas, we will start from the $p=1$ case and move to the large $p$ case. We will discuss model selection and evaluation, the bias-variance tradeoff, model stability, overfitting and regularization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aF9qC6dseeQt"
   },
   "outputs": [],
   "source": [
    "# Load the relevant modules\n",
    "import matplotlib.pylab as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_LZuhYGSeeQu"
   },
   "source": [
    "## Read the data\n",
    "\n",
    "We will start studying an artificial dataset to understand the methodology. We will move to real data and applications as we move along."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M7Wa4ZoFeeQu",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('https://github.com/barcelonagse-datascience/academic_files/raw/master/data/curve_data.csv')\n",
    "print(data.shape)\n",
    "data.head(10) # a first look at the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BzsceZ0Ka4Yb"
   },
   "source": [
    "In two dimensions $(p=1)$ we can easily visualize our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QHVC3y6NeeQu",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Simple plot to get a feel for the relationship between x and y.\n",
    "# plot here is a DF method\n",
    "data.plot(x='x',y='y',kind=\"scatter\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tosYXN77eeQu"
   },
   "source": [
    "## Our first learning function: linear in $\\bf x$\n",
    "We are interested in predicting $y_i$ given a sample of size $i=1,\\dots,n$ and some predictor variables $\\bf{x}_i$.\n",
    "In general, we are interested in finding the functional form of the conditional expectation of $y$ given $x$:\n",
    "\n",
    "$$ y_i = f(\\mathbf{x}_i , \\bb) + u_i ,$$\n",
    "\n",
    "and we call $f(\\mathbf{x}_i , \\bb)$  the *learning* or *regression function* . The regression function typically depends on the predictors, $\\mathbf{x}_i$ and on some typically unknown parameters, $\\bb$.\n",
    "\n",
    "We begin by considering learning functions that are linear in the parameters ($\\bb$) and in the predictors ($x_i$'s):\n",
    "$$ y_i = \\beta_0 + \\sum_{i=1}^{p}x_i\\beta_i + u_i $$\n",
    "or, equivalently,\n",
    "$$ y_i = {\\bf{x}}_i' {\\bb} + u_i $$\n",
    "where ${\\bf{x}_i} = (1, x_{i\\,1},\\dots,x_{i\\,p})'$ and $\\bb = (\\beta_0,\\dots,\\beta_{p})'$.\n",
    "\n",
    "## Estimating our linear function: Least Squares\n",
    "\n",
    "A typical way to _learn_ the parameters of the model is to estimate them based on some data. Typically, we will solve\n",
    "$$ \\hat \\bb = \\arg\\min_{\\bb} \\sum_{i=1}^n\\Big( y_i - \\bx_i' \\bb \\Big)^2 $$\n",
    "\n",
    "where $n$ is the number of observations in our sample.\n",
    "\n",
    "Hence, we can state that the line implied by the parameters $\\hat \\bb$ is the line that best fits our data cloud in the least square sense: it is the line for which the squared deviations are minimal.\n",
    "\n",
    "Minimizing least squares is a very simple problem. In particular, with some calculus, we can find closed form solutions for $\\hat \\bb$, or at least express it as the solution to a linear system of equations.\n",
    "\n",
    "In nice enough environments and denoting by $\\bf X$ and $\\bf y$ the $n\\times p$ matrix of $\\bf{x}_1,\\dots, \\bf{x}_n$ and the vector of $y_1,\\dots, y_n$, the closed form solution is $\\hat \\bb = (\\bf{X}'\\bf{X})^{-1}(\\bf{X}'\\bf{y})$.\n",
    "\n",
    "Moreover, under some conditions, the solution to the least squares problem is equivalent to that obtained by maximum likelihood (for example, under gaussianity of $u_i$).\n",
    "\n",
    "Remark on notation: bold-face for vectors, otherwise scalars; bold-face capital letters for matrices\n",
    "\n",
    "Lets see how to estimate a linear model in python.\n",
    "We first import the relevant tools. For predictive modelling, which is the aim here, `LinearRegression` is good enough. I would not use this for inference though. `statsmodels.api` would be a better choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "23SWUA36eeQu"
   },
   "outputs": [],
   "source": [
    "# importing the relevant sklearn tools\n",
    "#http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# Notice that a LinearRegression is a class\n",
    "reg  = LinearRegression(fit_intercept=True) # We create an instance of the LinearRegression class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EsLCXbGulz6d"
   },
   "outputs": [],
   "source": [
    "type(reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "noEYfvUrmjHn"
   },
   "outputs": [],
   "source": [
    "reg.fit(X=data.x, y=data.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FTJMfN5pmQL8"
   },
   "outputs": [],
   "source": [
    "type(data.x), data.x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IilYNthNeeQu"
   },
   "outputs": [],
   "source": [
    "# create predictors and response - and make sure they are\n",
    "# in the right format - sklearn prefers numpy arrays to pandas series.\n",
    "y = data.y.values.reshape(10,1) # we have to have everything in column vectors\n",
    "# You will sometimes see this done like this:\n",
    "X = data.x.values.reshape(-1,1)\n",
    "y.shape  , X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zczkcOnwA1tg"
   },
   "outputs": [],
   "source": [
    "data.y.values.reshape(-1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ixDYAjgDeeQu"
   },
   "outputs": [],
   "source": [
    "# We have previously created the reg instance of a LinearRegression class.\n",
    "# now, we use it to fit a model\n",
    "reg.fit(X=X, y=y)\n",
    "reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b0Vbp3uPTFcB"
   },
   "outputs": [],
   "source": [
    "reg.intercept_ , reg.coef_ # quick visualization of the estimated parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-f1nMUiAoKRR"
   },
   "outputs": [],
   "source": [
    "Xtilde = np.append(np.ones((10,1)), X,axis=1)\n",
    "Xtilde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8893OXZqpnO5"
   },
   "outputs": [],
   "source": [
    "XpX = np.linalg.inv(np.matmul(Xtilde.transpose(),Xtilde))\n",
    "Xpy = np.matmul(Xtilde.transpose(),y)\n",
    "np.matmul(XpX,Xpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJDsUMmqeeQu"
   },
   "source": [
    "### Predicting new data\n",
    "\n",
    "We will compute the *learning function* $f(\\bx,\\hat\\bb)$ on some *test data*. That is, given our estimates $\\hat \\bb$ (from the regression), we will compute $f( \\bx_o , \\hat{\\bb} )$, for some unseen $\\bx_o$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H02PQzhWeeQu"
   },
   "outputs": [],
   "source": [
    "# create the new points\n",
    "X_new = 0.01*np.arange(101).reshape(-1, 1) # https://numpy.org/doc/stable/reference/generated/numpy.reshape.html\n",
    "# the estimated function f(x,beta) at new inputs\n",
    "X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nvxqqlqvUZy0"
   },
   "outputs": [],
   "source": [
    "yhat_new = reg.predict(X_new) # predictions on the new data\n",
    "yhat_train = reg.predict(X) # predictions on the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bVHUN1eSa4Ye"
   },
   "source": [
    "What should we expect from this exercise? Well, we estimated the parameters of a linear model, so we expect our predictions to be _linear_ in X_new."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EB5imzKpa4Ye"
   },
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "plt.figure()\n",
    "# plot training data\n",
    "plt.scatter(X, y, c=\"orange\", label=\"Training data\", alpha=0.5)\n",
    "# plot predictions\n",
    "plt.plot(X_new, yhat_new, c=\"red\", label=\"Unseen data\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3vcu3X5Ya4Ye"
   },
   "source": [
    "This is the line that best fits our data..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mzBibBjPeeQu"
   },
   "source": [
    "## A more flexible learning function: linear in parameters and features, nonlinear in input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O3OYRqQ9eeQu"
   },
   "source": [
    "Clearly, a linear regression function is not able to capture the patterns in our curve data. We need a more flexible learning function. Consider:\n",
    "\n",
    "\n",
    "$$ y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\ldots +\\beta_n x_i^{p} $$\n",
    "\n",
    "We have added to our linear model _nonlinear_ transformations of our input $x$.\n",
    "\n",
    "This is a constructive perspective: we are creating **features**,  new input variables that are transformations of the original ones. In the above construction, if we let the vector of features for the $i$th data point be\n",
    "\n",
    "$$ {\\bf  F}_i =(1,x_i,x_i^2,\\ldots,x_i^{p})'$$\n",
    "\n",
    "then\n",
    "\n",
    "$$ f(x_i,\\bb) =  {\\bf  F}_i ' \\bb$$\n",
    "\n",
    "and this boils back to a standard regression, except that we now have $p+1$ predictors, even though $x$ is 1-dimensional. The choice of polynomial features is simply for illustration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RtPfa3JleeQu"
   },
   "outputs": [],
   "source": [
    "# Polynomial features are so common that sklearn has a built in function\n",
    "# for constructing them\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures as plf\n",
    "# the argument specifies the polynomial order, here we choose up to power 3\n",
    "poly = plf(3)\n",
    "F_med_train = poly.fit_transform(X) # F for feature matrix\n",
    "print(F_med_train) # notice that the intercept is now added by default (x^0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kAN_6JS3eeQu",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# We first fit our model\n",
    "reg_med = LinearRegression(fit_intercept=False) # Because we already have a column of 1's, we remove intercept\n",
    "reg_med.fit(X=F_med_train, y=y) # we fit the new model\n",
    "\n",
    "# Then we use it to create predictions. Note that we use the previously generated test data.\n",
    "F_med_new = plf(3).fit_transform(X_new) # Features\n",
    "yhat_med_new = reg_med.predict(F_med_new) # Predictions on the test data\n",
    "yhat_med_train = reg_med.predict(F_med_train) # Predictions on the training data\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X, y, c='orange', label='training data', alpha=0.5)\n",
    "plt.plot(X_new, yhat_med_new, c='red',label='new data')\n",
    "plt.plot(X,yhat_med_train,c='blue',label='insample fit')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XswJMm34a4Yg"
   },
   "source": [
    "Our more flexible polynomial is able to better capture the curvature of our data. But can we be _too_ flexible?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CoDylKCTa4Yg",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "F_large_train = plf(9).fit_transform(X) # F for feature matrix\n",
    "F_large_test = plf(9).fit_transform(X_new)\n",
    "\n",
    "# Estimate the model\n",
    "reg_large = LinearRegression(fit_intercept=False) # Because we already have a column of 1's, we remove intercept\n",
    "reg_large.fit(X=F_large_train, y=y)\n",
    "\n",
    "# compute predictions on unseen and train sample\n",
    "yhat_large_new = reg_large.predict(F_large_test)\n",
    "yhat_large_train = reg_large.predict(F_large_train)\n",
    "\n",
    "# plot\n",
    "plt.figure()\n",
    "plt.scatter(X, y, c='orange', label='training data', alpha=0.5)\n",
    "plt.plot(X_new, yhat_large_new, c='red',label='new data')\n",
    "plt.plot(X, yhat_large_train,c='blue',label='insample fit')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7xC9qiaD9AWC"
   },
   "outputs": [],
   "source": [
    "F_large_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iPczyOava4Yg"
   },
   "source": [
    "It looks like we are _overfitting_ our data.\n",
    "If we have 10 data points and 10 parameters to estimate, we will quite clearly fit perfectly in sample (the blue line perfectly lines up with the orange points).\n",
    "However, _in between_ our data points, we may not do a good job at predicting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DcYc74RTeeQu"
   },
   "source": [
    "## Model evaluation\n",
    "\n",
    "There are many ways to evaluate the quality of our model.\n",
    "Perhaps the most standard is the **square of the correlation coefficient** between our predictions and the realized values. This squared correlation coefficient is used so frequently that it has a name: R-squared. We can write it as follows\n",
    "\n",
    "$$ \\text{R-squared} = 1 - {\\sum_{i=1}^n (y_i - {\\bf x}_i'\\hat\\bb)^2 \\over \\sum_{i=1}^n (y_i - \\ybar)^2}= 1 - {SSR \\over TSS}$$\n",
    "\n",
    "Clearly, large R-squared is equivalent to a small **sum of squared residuals** (SSR: sum of squared residuals, TSS: total sum of squares). The better our models, the smallest the sum of squared residuals.\n",
    "\n",
    "Lets plot our data and the predictions implied by the least squares solution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1k_g8oQ1eeQv"
   },
   "outputs": [],
   "source": [
    "# plot predicted vs observed values for the simple linear model\n",
    "fig, (ax1, ax2,ax3) = plt.subplots(1, 3,figsize=(15,5),sharey='row')\n",
    "\n",
    "ax1.scatter(x=y,y=yhat_train)\n",
    "ax1.set_ylabel('Predicted')\n",
    "ax1.set_xlabel('Realized')\n",
    "ax1.plot(y,y,c=\"red\")\n",
    "rho = pd.Series(y[:,0]).corr(pd.Series(yhat_train[:,0]))\n",
    "ax1.set_title('R-squared for the simple model is %.3f' % rho**2 )\n",
    "\n",
    "ax2.scatter(x=y,y=yhat_med_train)\n",
    "ax2.plot(y,y,c=\"red\")\n",
    "rho = pd.Series(y[:,0]).corr(pd.Series(yhat_med_train[:,0]))\n",
    "ax2.set_title('R-squared for the medium model is %.3f' % rho**2 )\n",
    "ax2.set_xlabel('Realized')\n",
    "\n",
    "ax3.scatter(x=y,y=yhat_large_train)\n",
    "ax3.plot(y,y,c=\"red\")\n",
    "rho = pd.Series(y[:,0]).corr(pd.Series(yhat_large_train[:,0]))\n",
    "ax3.set_title('R-squared for the large model is %.3f' % rho**2 )\n",
    "ax3.set_xlabel('Realized')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B2SU2aoja4Yh"
   },
   "source": [
    "What is happening? We are computing the **insample** fit. As we add more parameters, our model becomes more flexible and the **insample** fit increases. In fact, when we have $p=n$, we achieve perfect fit. So the insample R-squared is _too optimistic_.\n",
    "\n",
    "In general, we are interested in predicting points that we have not yet seen. We need our measures of fit to be informative of how our model would perform in unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "momeNc2beeQv"
   },
   "source": [
    "Statistically speaking, letting $(x^*,y^*)$ be a randomly chosen *test* points from the same phenomenon that has generated the *training data* $(x_i,y_i),i=1,\\ldots,n$, we are interested in reliable estimates of\n",
    "\n",
    "+ Mean Squared Error (MSE):\n",
    "\n",
    "$$ \\text{MSE} = \\E\\Big[\\Big(y^* - {\\bf x^*}'\\hat\\bb\\Big)^2\\Big], \\text{ and }$$\n",
    "\n",
    "+ Squared correlation between $y^*$ and $\\hf_n(\\bx^*)$:\n",
    "\n",
    "$$ R^2 = \\Cor(y^*,{\\bf x^*}'\\hat\\bb)^2$$\n",
    "\n",
    "Where all the expectations are conditional on the training data.\n",
    "Up to now, we simply replaced the population expectation with the sample averages over the training data.\n",
    "\n",
    "However, we just saw that the insample version of these objects is too optimistic by construction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rKM9ncCHeeQv"
   },
   "source": [
    "Lets consider instead the **leave-one-out cross-validation estimator**. The intuition is very simple.\n",
    "Instead of using **all** our data to estimate the model, we:\n",
    "\n",
    "+ Use all data points but the $i$th to estimate $\\hat \\bb_{-i}$\n",
    "\n",
    "+ Using the estimated $\\hat \\bb{-i}$, compute the prediction for the (\"unseen\") $i$th training data point:\n",
    "\n",
    "$$ {\\bf x}_i \\hat \\bb_{-i}$$\n",
    "\n",
    "+ Estimate the MSE or $R^2$ on the _hold out_ sample (in this case, the $i$th observation)\n",
    "\n",
    "$$(y_i - {\\bf x}_i \\hat \\bb_{-i})^2$$\n",
    "\n",
    "+ We can do this for each data point $i$ and then average the estimates:\n",
    "\n",
    "$${1 \\over n} \\sum_{i=1}^n (y_i - {\\bf x}_i \\hat \\bb_{-i})^2$$\n",
    "\n",
    "We can implement this ourselves - or use some `sklearn` functions too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HOsIpmrEeeQv",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict as cvp\n",
    "yhat_cv  = cvp(reg, X, y, cv = 10) # this is leave-one-out CV when cv=10 and we set cv = 10 because n=10\n",
    "yhat_med_cv  = cvp(reg_med, F_med_train, y, cv = 10) # this is leave-one-out CV when cv=10 and we set cv = 10 because n=10\n",
    "yhat_large_cv  = cvp(reg_large, F_large_train, y, cv = 10) # this is leave-one-out CV when cv=10 and we set cv = 10 because n=10\n",
    "\n",
    "# plot predicted vs observed values\n",
    "fig, (ax1, ax2,ax3) = plt.subplots(1, 3,figsize=(15,5))\n",
    "\n",
    "ax1.scatter(x=y,y=yhat_cv)\n",
    "ax1.plot(y,y,c=\"red\")\n",
    "rho = pd.Series(y[:,0]).corr(pd.Series(yhat_cv[:,0]))\n",
    "ax1.set_title('LOOCV R-squared for the simple model is %.2f' % rho**2 )\n",
    "\n",
    "ax2.scatter(x=y,y=yhat_med_cv)\n",
    "ax2.plot(y,y,c=\"red\")\n",
    "rho = pd.Series(y[:,0]).corr(pd.Series(yhat_med_cv[:,0]))\n",
    "ax2.set_title('LOOCV R-squared for the medium model is %.2f' % rho**2 )\n",
    "\n",
    "ax3.scatter(x=y,y=yhat_large_cv)\n",
    "ax3.plot(y,y,c=\"red\")\n",
    "rho = pd.Series(y[:,0]).corr(pd.Series(yhat_large_cv[:,0]))\n",
    "ax3.set_title('LOOCV R-squared for the large model is %.2f' % rho**2 )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wresmli5aRmM"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict as cvp\n",
    "yhat_cv  = cvp(reg, X, y, cv = 10) # this is leave-one-out CV when cv=10 and we set cv = 10 because n=10\n",
    "yhat_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "INwpj32Ea4Yi"
   },
   "source": [
    "If we compare our Cross-Validated estimates with the insample estimates, we get substantial differences.\n",
    "\n",
    "It becomes very clear that the heavily parametrized model is very good insample, but very bad out of sample.\n",
    "Simple models may have a smaller R-squared in sample, but their performance is more stable.\n",
    "\n",
    "This leads us into the next topic:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xa45RUJ4eeQv"
   },
   "source": [
    "## The bias-variance tradeoff in Statistics and Machine Learning\n",
    "\n",
    "+ *Procedures* with *few degrees of freedom* are stable but the learning function they estimate can be systematically far off from the optimal one (**bias**). They would have comparable R-squared and leave-one-out CV R-squared\n",
    "\n",
    "\n",
    "+ *Procedures* with *high degrees of freedom* are overly sensitive to training data (which induces higher **variance**) but their flexibility allows them to approximate well the target series, and reduce bias. They would have near-1 R-squared and near-0 leave-one-out CV R-squared\n",
    "\n",
    "It is important to understand that these properties involve **both the model and the loss function** .\n",
    "In fact, the MSE can be written as the sum of the variance and the (squared) bias.\n",
    "\n",
    "This opens a lot of possibilities! In particular, we can trade-off bias and variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nu0E1a07eeQv"
   },
   "source": [
    "The following figure, taken from Bishop, shows the estimated root mean squared error - blue is in-sample, red is analogous to leave-one-out CV - for increasing values of $p$ (denoted by $M$ in the fig).\n",
    "\n",
    "<img src=\"https://github.com/barcelonagse-datascience/academic_files/raw/master/images/bishop_overfit.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R7f4qVfReeQv"
   },
   "source": [
    "We want **algorithms that can strike a good bias-variance tradeoff**!\n",
    "\n",
    "The remaining of this lecture is devoted to:\n",
    "\n",
    "1. Studying such algorithms: e.g. the LASSO; they use the same linear-in-features model but a different loss function\n",
    "2. Discussing how to estimate the MSE from data; we will revisit CV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xE92P5WBeeQv"
   },
   "source": [
    "## A framework for good predictive algorithms: shrinkage methods\n",
    "\n",
    "We now study *algorithms* that achieve a good bias-variance tradeoff and allow us to fit models with a very large number of features, potentially larger than the number of observations. The key structure that such algorithms try to exploit is **sparsity**, i.e., situations in which only a small number of features are relevant to get good predictions.\n",
    "\n",
    "If we **knew** what such features were, we could just select those and be done. But we typically do not know what they are and have to settle for a second best. One route is called **best subset selection** , meaning that we attempt to choose the best subset of size $k$ out of our original $p$ predictors. Without going into detail, I will just remark that this is a **really** hard and computationally demanding problem, so we typically go another route (although some alternatives exist: random subset, random projections,...)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lzXN49dxeeQv"
   },
   "source": [
    "\n",
    "These *algorithms* use the *same linear models* we have seen before. But they use slightly different loss functions.\n",
    "\n",
    "Recall that our methods so far focused on minimizing the square loss, _i.e_\n",
    "\n",
    "$$\\sum_{i=1}^n (y_i - \\bx_i' \\bb)^2$$\n",
    "\n",
    "The class of algorithms we discuss now are **shrinkage methods**; they are based on adapting the loss function to *shrink* coefficients in a meaningful direction. It turns out shrinking towards 0 is often a good choice (think of the sparsity notion that we just introduced), and we look for parameters such that\n",
    "\n",
    "$$ \\bb^g = \\arg\\min_\\bb \\sum_{i=1}^n (y_i - \\bx_i' \\bb)^2 + \\lambda \\sum_{j=1}^{p} g(\\beta_j)$$\n",
    "\n",
    "where $g(\\beta_j)$ is a **penalty** term, that penalizes $\\beta_j$ by $\\lambda\\geq0$ when $\\beta_j \\neq  0$; recall that $\\beta_j = 0$ means that feature $j$ (e.g., $j-1$ polynomial order, in the current case) is dropped from the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S-rgGN9veeQv"
   },
   "source": [
    "Procedures based on this type of loss functions are broadly called _penalized regression_.\n",
    "The following are some common examples of penalties - and the names the corresponding algorithms are known with:\n",
    "\n",
    "+ LASSO: $g(\\beta) = |\\beta|$ --> Shrinkage and Selection\n",
    "\n",
    "+ ridge regression: $g(\\beta) = \\beta^2$ --> Shrinkage (but no _hard_ 0s!)\n",
    "\n",
    "+ Elastic Net: If variables are strongly correlated, LASSO picks _some_ of them. Ridge, on the other hand, tends to shrink the coefficients of correlated variables to each other. The _Elastic Net_ is a compromise:\n",
    "\n",
    "$$ g(\\beta,\\alpha) = \\sum_{j=1}^p(\\alpha|\\beta_j| + (1-\\alpha)\\beta_j^2)$$\n",
    "the first term encourages a sparse solution and the second encourages highly correlated features to be averaged.\n",
    "\n",
    "+ The elastic net performs Shrinkage and Selection, and strikes a balance between the hard selection of the lasso and the averaging of similar coefficients of Ridge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-q2sDCWzyu0"
   },
   "source": [
    "All these models can be cast as restricted optimization problems. For instance, Ridge can be cast as\n",
    "$$ \\bb^{Ridge} = \\arg\\min_{\\bb} \\sum_{i=1}^n\\Big( y_i - \\bx_i' \\bb \\Big)^2$$\n",
    "subject to  $$  \\sum_{j=1}^{p} \\beta_j^2 < t $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WAJ2ip1L1B7r"
   },
   "source": [
    "Lasso can be cast as\n",
    "$$ \\bb^{LASSO} = \\arg\\min_{\\bb} \\sum_{i=1}^n\\Big( y_i - \\bx_i' \\bb \\Big)^2$$\n",
    "subject to  $$  \\sum_{j=1}^{p} |\\beta_j| < t $$\n",
    "\n",
    "In both cases, there is a one-to-one correspondence between $\\lambda$ and $t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XYvYNhfn2spt"
   },
   "source": [
    "In case of orthonormal predictors, we have that the coefficients for the lasso are given by\n",
    "\n",
    "$$ \\hat \\beta_j^{Ridge} = \\hat \\beta_j/(1 + \\lambda), \\hspace{2em} \\text{ and } \\hspace{2em} \\hat \\beta_j^{LASSO} = \\text{sign}(\\hat \\beta_j)(|\\hat \\beta_j - \\lambda|)_+ $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4H_iJk0Xunx3"
   },
   "source": [
    "A figure to help us see what is happening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lmKunWLislMW"
   },
   "outputs": [],
   "source": [
    "beta = np.arange(-3,3,0.0001)\n",
    "ridgePen = (beta**2)\n",
    "lassoPen = abs(beta)\n",
    "elnetPen = 0.5*ridgePen + 0.5*lassoPen\n",
    "plt.grid()\n",
    "plt.plot(beta,ridgePen, label='Ridge')\n",
    "plt.plot(beta,lassoPen,'r--',label='LASSO')\n",
    "plt.plot(beta,elnetPen,c='darkorange',linestyle='-.',label='Elastic Net')\n",
    "plt.scatter(0,0,s=70,c='r',marker='o')\n",
    "plt.ylim(-0.01,3)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j_6wmNsQeeQv"
   },
   "source": [
    "Some remarks:\n",
    "\n",
    "+ Feature standardization:\n",
    "\n",
    "a) Different coefficients are penalized in the same way: this only makes sense if the different coefficients have similar magnitudes.\n",
    "        \n",
    "b) Penalized likelihood algorithms require that the features have been standardized to have comparable scales. We often subtract the sample mean and divide by the standard deviation across replications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J9LTrAExeeQv"
   },
   "source": [
    "+ The role of $\\lambda$:\n",
    "    + This **hyperparameter** (a.k.a *tuning parameter*) allows us to trade bias with variance, creating a continuum of mean squared errors along which we try to choose an optimal $\\lambda$.\n",
    "    + $\\lambda \\to 0$ leads to small bias/large variance (there is no penalty), $\\lambda \\to \\infty$ to large bias/small variance (the model is effectively only estimating the constant term)\n",
    "    + Ridge regression with varying $\\lambda$s:\n",
    "    <img src=\"https://github.com/barcelonagse-datascience/academic_files/raw/master/images/bias_variance_bishop.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fTLZxu_seeQv"
   },
   "source": [
    "### Lasso in action: the curve data with many many features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3RNkU-FheeQv"
   },
   "outputs": [],
   "source": [
    "# standardisation of input is critical: We will use sklearn to do this\n",
    "from sklearn.preprocessing import scale as scl\n",
    "\n",
    "F_large_train  = scl(F_large_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P4BgOQdeeeQv"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "# alpha here is the tuning parameter\n",
    "lasso = Lasso( alpha = 0.00001, fit_intercept = False, warm_start = True , max_iter = 1000000) # small penalty\n",
    "lasso_aL = Lasso( alpha = 10e6, fit_intercept = False, warm_start = True , max_iter = 1000000) # very high penalty\n",
    "\n",
    "# application to our data and model\n",
    "lasso.fit(F_large_train , y)\n",
    "lasso_aL.fit(F_large_train , y)\n",
    "\n",
    "# see coefficients\n",
    "print(lasso.coef_ )\n",
    "print(lasso_aL.coef_)\n",
    "(np.abs(lasso.coef_).sum(), np.abs(lasso_aL.coef_).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C8AezqrH6XIr"
   },
   "outputs": [],
   "source": [
    "lasso.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "khJbQsw76faZ"
   },
   "outputs": [],
   "source": [
    "lasso_aL.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d-ee1YdJJsvZ"
   },
   "outputs": [],
   "source": [
    "(np.abs(lasso.coef_).sum(), np.abs(lasso_aL.coef_).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q7cFLmJKlke3"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "# alpha here is the tuning parameter\n",
    "ridge    = Ridge( alpha = 0.00001, fit_intercept = False, max_iter = 1000000) # small penalty\n",
    "ridge_aL = Ridge( alpha = 10e6, fit_intercept = False, max_iter = 1000000) # very high penalty\n",
    "\n",
    "# application to our data and model\n",
    "ridge.fit(F_large_train , y)\n",
    "ridge_aL.fit(F_large_train , y)\n",
    "\n",
    "# see coefficients\n",
    "print(ridge.coef_)\n",
    "print(ridge_aL.coef_ )\n",
    "\n",
    "( (ridge.coef_**2).sum(), (ridge_aL.coef_**2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4VEkzgMe7Mje"
   },
   "outputs": [],
   "source": [
    "ridge_aL.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OoDDAhB0KKFT"
   },
   "outputs": [],
   "source": [
    "( (ridge.coef_**2).sum(), (ridge_aL.coef_**2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XHrlkriqeeQv"
   },
   "outputs": [],
   "source": [
    "# CV R2\n",
    "yhat_lasso_cv = cvp(lasso, F_large_train, y, cv=10 )\n",
    "plt.figure()\n",
    "plt.scatter(x = y , y = yhat_lasso_cv)\n",
    "plt.plot(y , y , c = \"red\")\n",
    "rho = pd.Series(y[:,0]).corr(pd.Series(yhat_lasso_cv))\n",
    "plt.title('R-squared equals %.3f' % rho**2)\n",
    "plt.show()\n",
    "# No more overfitting! LASSO ``automatically'' reduced the dimension of our model from 10 to 5.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Gb0mLONeeQv"
   },
   "source": [
    "### Further insights & observations on LASSO\n",
    "\n",
    "+ Sparsity: increasing values of $\\lambda$ have the effect that an increasing number of estimated coefficients are exactly zero\n",
    "+ Variable selection: hence, implictly lasso also performs a principled feature selection - but this is not an aspect we will explore here\n",
    "    + Lets see these properties in action in our example. Lets look at the coefficients for a range of $\\lambda$ values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3UwcWOhpeeQv"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import lars_path\n",
    "\n",
    "alphas, _, coefs = lars_path(F_large_train, y[:,0], method='lasso',\n",
    "                             verbose=True, max_iter = 100000)\n",
    "\n",
    "xx  = np.sum( np.abs( coefs ), axis = 0 ) # sum of absolute coefficients\n",
    "xx /= xx[-1]                              # standardized by the maximum.\n",
    "\n",
    "plt.plot(xx, coefs.T)\n",
    "\n",
    "ymin, ymax = plt.ylim()\n",
    "plt.vlines(xx, ymin, ymax, linestyle='dashed')\n",
    "plt.xlabel('|coef| / max|coef|')\n",
    "plt.ylabel('Coefficients')\n",
    "plt.title('LARS Path')\n",
    "plt.axis('tight')\n",
    "plt.xlim(0,0.001)\n",
    "plt.ylim(-5,5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lDcXMnuSeeQw"
   },
   "source": [
    "+ Convexity: the loss function is convex; this is because the least squares function is convex (a quadratic function) and the penalty is convex too. This allows very efficient estimation using **convex optimization** algorithms.\n",
    "    + A common choice is **coordinate-wise descent**. This is an iterative algorithm that scans through each coefficient and updates it using information about the values of all other coefficients.\n",
    "    + For standardized features $\\bx_1,\\bx_2,\\ldots$ each coefficient is updated as:\n",
    "$$\\beta_j \\leftarrow \\mathcal{S}_{\\lambda}\\left({1 \\over n} \\boldsymbol{r}_{-j}^T \\bx_j\\right )$$\n",
    "where $\\boldsymbol{r}_{-j}$ is the vector of residuals from the model with $\\beta_j = 0$ and the soft-thresholding operator is:\n",
    "      $$\\mathcal{S}_{\\lambda}(\\beta) = \\mathrm{sign}(\\beta) \\max\\{|\\beta| - \\lambda,0\\}$$\n",
    "    + The fast optimization is a major attraction for the lasso\n",
    "      + Coordinate-wise descent is implemented at a cost that grows only linearly in $n$ and $p$: it is a practical solution for Big Data and Big Models  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Ox1KpIueeQw"
   },
   "source": [
    "## Choosing the regularization hyperparameter\n",
    "\n",
    "Given an estimator of the MSE (*i.e.* LOOCV), we can choose $\\lambda$ to hopefully achieve small MSE.\n",
    "Another possibility is to use a **model selection** criterion. Model selection criteria balance in-sample fit with **model complexity**.\n",
    "\n",
    "First, we try leave-one-out CV in our example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q8ff1sUVeeQw"
   },
   "source": [
    "### Leave-one-out CV selection of $\\lambda$ for the curve data\n",
    "\n",
    "Lets try and do this using the leave-one-out CV we have already discussed. We will try a range of different $\\lambda$s, for each of which we will estimate the MSE by leave-one-out CV, plot the resultant curve and try to identify a good $\\lambda$\n",
    "\n",
    "The procedure is computationally intensive - this will not manifest here where $n=10$\n",
    "\n",
    "We will use `GridSearchCV` to carry out the outer (grid search and CV) loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NFZ6Zk1geeQw"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "lasso  = Lasso(random_state=0,max_iter=3000000)\n",
    "alphas = np.array(\n",
    "    [0.000007, 0.00002, 0.00004, 0.00005,\n",
    "     0.00008,0.0001,0.00012, 0.00015,0.0002,0.00025,0.0003,0.0004,0.0005,0.0006,0.0007,0.002])\n",
    "\n",
    "tuning_parameters = [{'alpha': alphas}]\n",
    "n_folds = 10 # remember that for this dataset this is leave-one-out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gUoMtLAmeeQw"
   },
   "outputs": [],
   "source": [
    "# create a scorer to evaluate performance\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "\n",
    "## ALWAYS read carefully documentation. copying here from make_scorer\n",
    "## greater_is_better : boolean, default=True\n",
    "# \"Whether score_func is a score function (default), meaning high is\n",
    "# good, or a loss function, meaning low is good.\n",
    "# In the latter case, the scorer object will sign-flip\n",
    "# the outcome of the score_func.\n",
    "mse = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "\n",
    "clf = GridSearchCV(lasso, tuning_parameters, scoring = mse,\n",
    "                   cv = n_folds, refit = True)\n",
    "\n",
    "clf.fit(F_large_train, y)\n",
    "scores = clf.cv_results_['mean_test_score']\n",
    "scores_std = clf.cv_results_['std_test_score']\n",
    "std_error = scores_std / np.sqrt(n_folds)\n",
    "pd.DataFrame(-scores , alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FQdAcarVU1iG"
   },
   "outputs": [],
   "source": [
    "# Extract best param\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d4NlViG3eeQw"
   },
   "outputs": [],
   "source": [
    "# Plotting the results\n",
    "\n",
    "plt.figure().set_size_inches(8, 6)\n",
    "plt.semilogx(alphas, -scores)\n",
    "\n",
    "# plot error lines showing +/- std. errors of the scores\n",
    "plt.semilogx(alphas, -scores + std_error, 'b--')\n",
    "plt.semilogx(alphas, -scores - std_error, 'b--')\n",
    "\n",
    "# alpha = 0.2 controls the translucency of the fill color\n",
    "plt.fill_between(alphas, -scores + std_error, -scores - std_error, alpha=0.2)\n",
    "plt.ylabel('CV score +/- std error')\n",
    "plt.xlabel('alpha')\n",
    "plt.axhline(np.min(-scores), linestyle='--', color='.5')\n",
    "ymin, ymax = plt.ylim()\n",
    "plt.vlines(clf.best_params_['alpha'] ,ymin, ymax, linestyle='dashed')\n",
    "plt.xlim([alphas[0], alphas[-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xwjnADJueeQw"
   },
   "source": [
    "## Some hints\n",
    "\n",
    "\n",
    "+ Building good predictive models with hundreds or even thousands of features is a real possibility\n",
    "+ LASSO combines least squares with a penalty for model complexity; it relies on an additional *regularization parameter*\n",
    "+ Sklearn module `LinearRegression` can be used for predictive modeling. `Lasso` can be used to fit a lasso model for given value of regularization hyperparameter. `lars_path` can return all the possible lasso solutions for all values of the regularization hyperparameter and is a useful tool in exploring the different models\n",
    "+ The choice of regularization hyperparameter is a model selection problem; you can use both cross validation to estimate the MSE for each possible value of the hyperparameter and use a grid search to identify good values for the hyperparameter - `GridSearchCV` is useful wrapper for this. Less data and computationally intensive method is to use a model selection criterion, e.g. AIC, and a simple formula exists for the lasso\n",
    "+ For inference with a linear model, i.e., obtaining confidence intervals, p-values etc, `LinearRegression` is  entirely inappropriate. Use other modules, e.g., `statsmodels.api`.\n",
    "+ Inference with the output of the lasso model is non-trivial and subject of more advanced material. Although lasso implicitly selects a model by dropping variables, you should not over-interpret the variables that have been selected. Its merit is primary in getting a good predictive model. Lasso is helpful in screening some variables, so it is often used as a first step to be followed by a more formal selection procedure. Generally, these questions fall under the theme of *post-selection* inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ow7uT1iNeeQw"
   },
   "source": [
    "## References\n",
    "\n",
    "Hastie, T., Tibshirani, R., Friedman, J., 2009. *Elements of Statistical Learning*. 2nd Edition. Chapters 1, 2, 3. Section 3.4; More advanced 3.8,3.9,7.10  https://web.stanford.edu/~hastie/ElemStatLearn/\n",
    "\n",
    "Bishop, C.M. *Pattern recognition and machine learning*. Chapter 1, Sections 3.1, 3.2"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
